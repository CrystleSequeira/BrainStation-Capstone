{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Machine Learning - Aproach 1 & 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the wine scores, I used three classification models: Logistic Regression, Random Forest, and XGboost.\n",
    "\n",
    "This jupyter notebook is my initial attempt at predicting the wine scores. Complications became apparent when running the machine learning algorthims. I had realized that I had setup the train/set dataset incorrectly from the Part 2 jupyter notebook. Approach 1 and 2 methods for handling the one hot encoding process are described in the AStern_Part2_NLP_Wine_Reviews.ipynb for reference. \n",
    "\n",
    "I am providing this notebook as a reference in providing all of my steps and workflow processes. This jupyter notebook is not to be used for future work. Instead, use AStern_Part3B_ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For splitting our data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For some simple model building\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# This gets rid of those annoying default solver messages when fitting logistic regression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# For setting up a temporary directory for caching pipeline results\n",
    "from tempfile import mkdtemp\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Some scalers we'll try later\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# For trying PCA later\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# For cross-validated grid search\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "train = pd.read_csv('data/model_train.csv')\n",
    "test = pd.read_csv('data/model_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is train datasett\n",
    "X = train.drop('points', axis = 1)\n",
    "y = train['points']\n",
    "\n",
    "# This is train datasett\n",
    "X_test = test.drop('points', axis = 1)\n",
    "y_test = test['points']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2 for NLP\n",
    "\n",
    "This dataset is from when I used Approach 2 to transform the Country, Province, and Variety into numeric values through get_dummies. I created a threshold to only include contries, provinces, and wine varieties that frequently appeared in the dataset. By doing this the dataset reduced from 42 countries to 12 countries, 406 provinces to 34 provinces, and 646 varieties to only 34 varieties! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on remainder set: 0.5322905490616824\n",
      "Accuracy on remainder set: 0.5276013678553981\n"
     ]
    }
   ],
   "source": [
    "# Baseline logistic regression\n",
    "baseline_logreg = LogisticRegression(random_state=1).fit(X_train, y_train)\n",
    "\n",
    "print(f'Accuracy on remainder set: {baseline_logreg.score(X_train, y_train)}')\n",
    "print(f'Accuracy on remainder set: {baseline_logreg.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score for the train set is 53.2% and the accuracy score for the test set is 52.7%. This is not comforting to see as the original accuracy score was 80% from Approach 1. As well an accuraacy score of 50% is not beneficial for predicing wine scores espcially when there are only two classifications to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete a little more further anlysis with hyperparameterization train and test dataset from Approach 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deteremine which c_value is best for a logistic regression model\n",
    "c_values = [.00001, .0001, .001, .1, 1, 10, 100, 1000, 10000]\n",
    "\n",
    "# Set up empty lists\n",
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "\n",
    "# For loop\n",
    "for value in c_values:\n",
    "    \n",
    "    # Instantiate log reg and fit to train set\n",
    "    logreg = LogisticRegression(C=value, random_state=1).fit(X_train, y_train)\n",
    "    \n",
    "    # Score on train set and append accuracy\n",
    "    train_accuracies.append(logreg.score(X_train, y_train))\n",
    "    \n",
    "    # Score on validation set and append accuracy\n",
    "    validation_accuracies.append(logreg.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C value</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>validation accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.528658</td>\n",
       "      <td>0.528685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.528658</td>\n",
       "      <td>0.528685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.528658</td>\n",
       "      <td>0.528685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.532272</td>\n",
       "      <td>0.528836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.532291</td>\n",
       "      <td>0.528685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.00000</td>\n",
       "      <td>0.532328</td>\n",
       "      <td>0.528610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.532366</td>\n",
       "      <td>0.528610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1000.00000</td>\n",
       "      <td>0.534022</td>\n",
       "      <td>0.525448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10000.00000</td>\n",
       "      <td>0.532366</td>\n",
       "      <td>0.528610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       C value  train accuracy  validation accuracy\n",
       "0      0.00001        0.528658             0.528685\n",
       "1      0.00010        0.528658             0.528685\n",
       "2      0.00100        0.528658             0.528685\n",
       "3      0.10000        0.532272             0.528836\n",
       "4      1.00000        0.532291             0.528685\n",
       "5     10.00000        0.532328             0.528610\n",
       "6    100.00000        0.532366             0.528610\n",
       "7   1000.00000        0.534022             0.525448\n",
       "8  10000.00000        0.532366             0.528610"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the accuracies\n",
    "pd.DataFrame({'C value': c_values, 'train accuracy': train_accuracies, 'validation accuracy': validation_accuracies})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the accuracies\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(c_values, train_accuracies, label='Train', marker='.')\n",
    "plt.plot(c_values, validation_accuracies, label='Validation', marker='.')\n",
    "plt.legend()\n",
    "plt.title('Effect of Varying C on Logistic Regression Accuracies')\n",
    "plt.xscale('log') # This is important for making the plot much more readable\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset C = .001 however this is for when cross validation is not completed or scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This lets us \"pickle\" things, like accuracies in this case or even an entire fitted model \n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see if the model will improve when fed through a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   14.5s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   16.3s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   18.0s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:   20.0s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   22.2s\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:   24.7s\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:   27.5s\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:   30.6s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:   33.5s\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:   39.4s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   46.4s\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed:   50.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   54.3s\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed:   58.9s\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 330 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 413 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 473 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 537 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 570 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 605 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 677 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 714 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 753 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 810 out of 810 | elapsed:  5.0min finished\n"
     ]
    }
   ],
   "source": [
    "# Set up a directory to cache the pipeline results\n",
    "cachedir = mkdtemp()\n",
    "\n",
    "# Set up a pipeline\n",
    "# The steps here act as placeholders and will be changed when we pass the pipeline into the grid search later\n",
    "my_pipeline = Pipeline([('scaler', StandardScaler()), ('dim_reducer', PCA()), ('model', LogisticRegression())], memory=cachedir)\n",
    "\n",
    "# Let's try the same range of C values from earlier\n",
    "c_values = [.00001, .0001, .001, .1, 1, 10, 100, 1000, 10000]\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = [\n",
    "    \n",
    "    # l2 (default) with PCA\n",
    "    {'scaler': [None, StandardScaler(), RobustScaler()],\n",
    "     'dim_reducer': [PCA()],\n",
    "     'dim_reducer__n_components': [10, 20, 40, 60, 80, 100],\n",
    "     'model': [LogisticRegression(solver='lbfgs', random_state=1, n_jobs=-1)],\n",
    "     'model__C': c_values}\n",
    "]\n",
    "\n",
    "# Instantiate the log reg grid search\n",
    "logreg_gs = GridSearchCV(my_pipeline, param_grid=param_grid, n_jobs = -1, cv=5, verbose=10)\n",
    "\n",
    "# Fit the log reg grid search\n",
    "fitted_logreg_gs = logreg_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory='/var/folders/5y/d61vfhqx3bv7m_5qfsqw1g8c0000gn/T/tmphlar0iv1',\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('dim_reducer',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=10,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('model',\n",
       "                 LogisticRegression(C=1e-05, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=-1, penalty='l2',\n",
       "                                    random_state=1, solver='lbfgs', tol=0.0001,\n",
       "                                    verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best model attributes\n",
    "fitted_logreg_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best estimator model is when a StandardScaler is used, PCA n_components = 10, and C = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5286577446496132"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy score for train data\n",
    "fitted_logreg_gs.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5286854389399187"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy score for validation data\n",
    "fitted_logreg_gs.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after running a pipline which can incorporate scaling and cross validation at the same time, the accuracy score is still only 52%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a directory to cache the pipeline results\n",
    "cachedir = mkdtemp()\n",
    "\n",
    "# Set up a pipeline\n",
    "# The steps here act as placeholders and will be changed when we pass the pipeline into the grid search later\n",
    "my_pipeline = Pipeline([('scaler', StandardScaler()), ('dim_reducer', PCA()), ('model', LogisticRegression())], memory = cachedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try with a random forest through a pipeline to see any improvements in the accruacy score will be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   22.8s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   40.3s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed: 10.0min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed: 12.9min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed: 18.1min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed: 30.5min\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed: 37.0min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 38.4min\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed: 40.7min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 45.1min\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed: 52.3min\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed: 59.5min\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed: 110.0min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed: 273.3min\n",
      "[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed: 284.2min\n",
      "[Parallel(n_jobs=-1)]: Done 330 tasks      | elapsed: 307.1min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 388.0min\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed: 505.7min\n",
      "[Parallel(n_jobs=-1)]: Done 413 tasks      | elapsed: 593.5min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 596.7min\n",
      "[Parallel(n_jobs=-1)]: Done 473 tasks      | elapsed: 610.4min\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed: 627.0min\n",
      "[Parallel(n_jobs=-1)]: Done 537 tasks      | elapsed: 656.1min\n",
      "[Parallel(n_jobs=-1)]: Done 570 tasks      | elapsed: 661.5min\n",
      "[Parallel(n_jobs=-1)]: Done 605 tasks      | elapsed: 673.7min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 696.3min\n",
      "[Parallel(n_jobs=-1)]: Done 677 tasks      | elapsed: 727.5min\n",
      "[Parallel(n_jobs=-1)]: Done 714 tasks      | elapsed: 735.2min\n",
      "[Parallel(n_jobs=-1)]: Done 753 tasks      | elapsed: 758.2min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 795.0min\n",
      "[Parallel(n_jobs=-1)]: Done 810 out of 810 | elapsed: 813.8min finished\n"
     ]
    }
   ],
   "source": [
    "# n_estimators\n",
    "n_estimator = [10, 20, 40, 60, 80, 100, 120, 140, 160]\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = [\n",
    "\n",
    "    # l2 (default) with PCA\n",
    "    {'scaler': [None, StandardScaler(), RobustScaler()],\n",
    "     'dim_reducer': [PCA()],\n",
    "     'dim_reducer__n_components': [10, 20, 40, 60, 80, 100],\n",
    "     'model': [RandomForestClassifier(n_jobs = -1)],\n",
    "     'model__n_estimators': n_estimator}\n",
    "]\n",
    "\n",
    "# Instantiate the log reg grid search\n",
    "random_forest_gs = GridSearchCV(my_pipeline, param_grid=param_grid, n_jobs = -1, cv=5, verbose=10)\n",
    "\n",
    "# Fit the log reg grid search\n",
    "fitted_randomforest_gs = random_forest_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory='/var/folders/5y/d61vfhqx3bv7m_5qfsqw1g8c0000gn/T/tmpkaj9ki6i',\n",
       "         steps=[('scaler', None),\n",
       "                ('dim_reducer',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=100,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('model',\n",
       "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                        criterion='gini', max_depth=None,\n",
       "                                        max_features='auto',\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=140, n_jobs=-1,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_randomforest_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best estimator for a random forest is when n_components = 100, scaler is none, and n_estimator = 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9779584768573418"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy Score for tain\n",
    "fitted_randomforest_gs.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5163379009185364"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy score for random forest\n",
    "fitted_randomforest_gs.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5122129946262823"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy score for test\n",
    "fitted_randomforest_gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that the accuracy score is a 97% for the train dataset, which might seem good but by looking at the accuracy scores for the test and validation set we see that the accuracy score is 51% which means that the model is overfitting for the train dataset. Therefore the random forest was not an improvement from the logistic regression.\n",
    "\n",
    "I did not run a XGboost model on this dataset because while XGboost could potentially improve the model, it is not going to improve the model by 30-40% which is what I would like to see. Therefore we can determine that removing the variation in the countries, province, and variety hindered the accuracy score. Back to the drawing board!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipline/Gridsearch to find best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a directory to cache the pipeline results\n",
    "cachedir = mkdtemp()\n",
    "\n",
    "my_pipeline = Pipeline([('scaler', StandardScaler()), ('dim_reducer', PCA()), ('model', LogisticRegression())], memory = cachedir)\n",
    "# Let's try the same range of C values from earlier\n",
    "n_estimator = [300]\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = [\n",
    "\n",
    "    # l2 (default) with PCA\n",
    "    {'scaler': [None, StandardScaler(), RobustScaler()],\n",
    "     'dim_reducer': [PCA()],\n",
    "     'dim_reducer__n_components': [100],\n",
    "     'model': [RandomForestClassifier(n_jobs = -1)],\n",
    "     'model__n_estimators': n_estimator}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the log reg grid search\n",
    "random_forest_gs = GridSearchCV(my_pipeline, param_grid=param_grid, n_jobs = -1, cv=5, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed: 25.4min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  15 | elapsed: 40.5min remaining: 20.3min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  15 | elapsed: 42.2min remaining: 10.6min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed: 48.6min finished\n"
     ]
    }
   ],
   "source": [
    "# Fit the log reg grid search\n",
    "fitted_randomforest_gs = random_forest_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9999862605278705\n",
      "valdiation accuracy: 0.8039679050340734\n"
     ]
    }
   ],
   "source": [
    "print('train accuracy:', fitted_randomforest_gs.score(X_train, y_train))\n",
    "print('valdiation accuracy:', fitted_randomforest_gs.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy score of train set is 99% and validation score is 80% which indicates that the model is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7972, 1637],\n",
       "       [1785, 6802]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## CONFUSION MATRIX\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = fitted_randomforest_gs.predict(X_val)\n",
    "confusion_matrix(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix provides insights into how well the model performed in terms of predicting True positives and true negatives. We can see from the above tha the model correctly predicted True negatives (where points = 0) of 7972 and incorrectly predicted 1785 reviews as negative. The model correctly predicted 6802 as positive scores and incorectly predictetd 1637 as positive scores.\n",
    "\n",
    "As expected the model did a better job at predicting the negative scores than the positive scores (explained in Part 2 of the project)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8119366893822818"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Accuracy Score for validation set\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8060196705770826"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision Score for Validation Set\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# precision_score(true labels, predicted labels)\n",
    "precision_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7921276347967858"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall Score for Validation set\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# recall_score(true labels, predicted labels)\n",
    "recall_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View in a plot how the y validation data is predicted.\n",
    "plt.figure()\n",
    "plt.hist(Ypredict)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy score for Test dataset cannot be completed because the X_test has a different size matrix than the train dataset because the train and test set contain different features. The different features are from when splitting the train and test set, there were "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (38992,1348) (1325,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-aa5102bf0b97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfitted_randomforest_gs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    426\u001b[0m                              % self.best_estimator_)\n\u001b[1;32m    427\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorer_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultimetric_\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorer_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36m_passthrough_scorer\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_passthrough_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;34m\"\"\"Function that wraps estimator.score\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m             \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0mscore_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mX_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhiten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (38992,1348) (1325,) "
     ]
    }
   ],
   "source": [
    "fitted_randomforest_gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error indicates that the Test size and Train size are not the same shape. We can see this below as well. While the rows do not need to be the same size, the columns and feature names need to be the same for test and train dataset. This is something that I did not consider checking when I ran the one hot encoding. For tfidf this is not a problem because I fit the tfidf vectorizer to the train set and then transform the test set from the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38992, 1132)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72783, 1325)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed: 10.3min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed: 11.1min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed: 12.5min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed: 13.6min\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed: 14.8min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed: 17.0min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 18.5min\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed: 19.8min\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed: 21.3min\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed: 23.0min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed: 28.9min\n",
      "[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed: 31.0min\n",
      "[Parallel(n_jobs=-1)]: Done 330 tasks      | elapsed: 32.5min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 34.2min\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed: 35.8min\n",
      "[Parallel(n_jobs=-1)]: Done 413 tasks      | elapsed: 37.9min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 70.5min\n",
      "[Parallel(n_jobs=-1)]: Done 473 tasks      | elapsed: 174.3min\n",
      "[Parallel(n_jobs=-1)]: Done 495 out of 495 | elapsed: 218.5min finished\n"
     ]
    }
   ],
   "source": [
    "# Set up a directory to cache the pipeline results\n",
    "cachedir = mkdtemp()\n",
    "\n",
    "# Set up a pipeline\n",
    "# The steps here act as placeholders and will be changed when we pass the pipeline into the grid search later\n",
    "my_pipeline = Pipeline([('scaler', StandardScaler()), ('dim_reducer', PCA()), ('model', LogisticRegression())], memory = cachedir)\n",
    "\n",
    "\n",
    "# Let's try the same range of C values from earlier\n",
    "c_values = c_values = [.00001, .0001, .001, .1, 1, 10, 100, 1000, 10000]\n",
    "\n",
    "\n",
    "# Parameter grid\n",
    "logreg_param_grid = [\n",
    "\n",
    "    # l2 (default) with PCA\n",
    "    {'scaler': [RobustScaler()],\n",
    "     'dim_reducer': [PCA()],\n",
    "     'dim_reducer__n_components': [1, 10, 100, 200, 300, 400],\n",
    "     'model': [LogisticRegression(solver='lbfgs', random_state=1, n_jobs=-1)],\n",
    "     'model__C': c_values},\n",
    "    \n",
    "]\n",
    "\n",
    "# Instantiate the log reg grid search\n",
    "logreg_gs = GridSearchCV(my_pipeline, param_grid=logreg_param_grid, n_jobs = -1, cv=5, verbose=10)\n",
    "\n",
    "# Fit the log reg grid search\n",
    "fitted_logreg_gs = logreg_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory='/var/folders/5y/d61vfhqx3bv7m_5qfsqw1g8c0000gn/T/tmpxxhifh7x',\n",
       "         steps=[('scaler',\n",
       "                 RobustScaler(copy=True, quantile_range=(25.0, 75.0),\n",
       "                              with_centering=True, with_scaling=True)),\n",
       "                ('dim_reducer',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=300,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('model',\n",
       "                 LogisticRegression(C=1000, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=-1, penalty='l2',\n",
       "                                    random_state=1, solver='lbfgs', tol=0.0001,\n",
       "                                    verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_logreg_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best logistic regression's accuracy on the remainder set: 0.7933995575889974\n",
      "The best logistic regression's accuracy on the test set: 0.7908331501428886\n"
     ]
    }
   ],
   "source": [
    "print(f\"The best logistic regression's accuracy on the remainder set: {fitted_logreg_gs.score(X_train, y_train)}\")\n",
    "print(f\"The best logistic regression's accuracy on the test set: {fitted_logreg_gs.score(X_val, y_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running many different models from, I was able to get the best_estimator for logistic regression. I did not originally save the model as a pkl therefore I needed to run the best fitting model again to be able to evaluate the model with precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  1.9min remaining:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:  1.9min remaining:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.5min finished\n"
     ]
    }
   ],
   "source": [
    "# Set up a directory to cache the pipeline results\n",
    "cachedir = mkdtemp()\n",
    "\n",
    "# Set up a pipeline\n",
    "# The steps here act as placeholders and will be changed when we pass the pipeline into the grid search later\n",
    "my_pipeline = Pipeline([('scaler', StandardScaler()), ('dim_reducer', PCA()), ('model', LogisticRegression())], memory = cachedir)\n",
    "\n",
    "\n",
    "# Let's try the same range of C values from earlier\n",
    "c_values = [1000]\n",
    "\n",
    "\n",
    "# Parameter grid\n",
    "logreg_param_grid = [\n",
    "\n",
    "    # l2 (default) with PCA\n",
    "    {'scaler': [RobustScaler()],\n",
    "     'dim_reducer': [PCA()],\n",
    "     'dim_reducer__n_components': [300],\n",
    "     'model': [LogisticRegression(solver='lbfgs', random_state=1, n_jobs=-1)],\n",
    "     'model__C': c_values},\n",
    "    \n",
    "]\n",
    "\n",
    "# Instantiate the log reg grid search\n",
    "logreg_gs = GridSearchCV(my_pipeline, param_grid=logreg_param_grid, n_jobs = -1, cv=5, verbose=10)\n",
    "\n",
    "# Fit the log reg grid search\n",
    "fitted_logreg_gs = logreg_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7940040943626946"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy Score of the model from the train dataset\n",
    "fitted_logreg_gs.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score for logistic regression 0.791272807210376\n",
      "precision score for logistic regression 0.7871447415757286\n",
      "recall score for logistic regression 0.7644113194363573\n"
     ]
    }
   ],
   "source": [
    "# Accuracy scores for logistic regression for validation set:\n",
    "y_pred = fitted_logreg_gs.predict(X_val)\n",
    "print('accuracy score for logistic regression', fitted_logreg_gs.score(X_val, y_val))\n",
    "# precision_score(true labels, predicted labels)\n",
    "print('precision score for logistic regression', precision_score(y_val, y_pred))\n",
    "# recall_score(true labels, predicted labels)\n",
    "print('recall score for logistic regression', recall_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 76.9min\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  30 | elapsed: 121.5min remaining: 13.5min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 124.4min finished\n"
     ]
    }
   ],
   "source": [
    "# Set up a directory to cache the pipeline results\n",
    "cachedir = mkdtemp()\n",
    "\n",
    "# The steps here act as placeholders and will be changed when we pass the pipeline into the grid search later\n",
    "my_pipeline = Pipeline([('scaler', StandardScaler()), ('dim_reducer', PCA()), ('model', LogisticRegression())], memory = cachedir)\n",
    "\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = [\n",
    "    \n",
    "    # XGB Boost\n",
    "    {'scaler': [StandardScaler(), RobustScaler()],\n",
    "     'dim_reducer': [PCA()],\n",
    "     'dim_reducer__n_components': [100, 200, 300],\n",
    "     'model': [XGBClassifier()]}\n",
    "    \n",
    "]\n",
    "\n",
    "# Instantiate the log reg grid search\n",
    "xgboost_gs = GridSearchCV(my_pipeline, param_grid=param_grid, n_jobs = -1, cv=5, verbose=10)\n",
    "\n",
    "# Fit the log reg grid search\n",
    "fitted_xgboost_gs = xgboost_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory='/var/folders/5y/d61vfhqx3bv7m_5qfsqw1g8c0000gn/T/tmpnbrfy5v_',\n",
       "         steps=[('scaler',\n",
       "                 RobustScaler(copy=True, quantile_range=(25.0, 75.0),\n",
       "                              with_centering=True, with_scaling=True)),\n",
       "                ('dim_reducer',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=300,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('model',\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "                               max_delta_step=0, max_depth=3,\n",
       "                               min_child_weight=1, missing=None,\n",
       "                               n_estimators=100, n_jobs=1, nthread=None,\n",
       "                               objective='binary:logistic', random_state=0,\n",
       "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "                               seed=None, silent=None, subsample=1,\n",
       "                               verbosity=1))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_xgboost_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation for XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score for logistic regression 0.7758298527148824\n",
      "precision score for logistic regression 0.7652388797364086\n",
      "recall score for logistic regression 0.7573075579364155\n"
     ]
    }
   ],
   "source": [
    "# Accuracy scores for logistic regression for validation set:\n",
    "y_pred = fitted_xgboost_gs.predict(X_val)\n",
    "print('accuracy score for logistic regression', fitted_xgboost_gs.score(X_val, y_val))\n",
    "# precision_score(true labels, predicted labels)\n",
    "print('precision score for logistic regression', precision_score(y_val, y_pred))\n",
    "# recall_score(true labels, predicted labels)\n",
    "print('recall score for logistic regression', recall_score(y_val, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
